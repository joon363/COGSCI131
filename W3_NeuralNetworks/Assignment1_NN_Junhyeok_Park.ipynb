{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #Define Parameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize=1\n",
    "        self.hiddenLayerSize=3\n",
    "        \n",
    "        #Define Weights\n",
    "        self.W1=np.random.rand(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2=np.random.rand(self.hiddenLayerSize,self.outputLayerSize)\n",
    "\n",
    "    def forward(self,X):\n",
    "        #Propagate inputs through network\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "    \n",
    "    def forward_ReLU(self,X):\n",
    "        #Propagate inputs through network\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply Sigmoid Activation Function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Derivative of Sigmoid Function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def reluPrime(self,z):\n",
    "        return 1 * (z > 0)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute Cost Function with weights already stored in class\n",
    "        self.yHat=self.forward(X)\n",
    "        J=0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def costFunction_ReLU(self, X, y):\n",
    "        #Compute Cost Function with weights already stored in class\n",
    "        self.yHat=self.forward_ReLU(X)\n",
    "        J=0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivatives with respect to W1 and W2\n",
    "        self.yHat=self.forward(X)\n",
    "        delta3 = np.multiply(-(y-self.yHat),self.sigmoidPrime(self.z3))\n",
    "        dJdW2=np.dot(self.a2.T,delta3)\n",
    "        delta2=np.dot(delta3,self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1=np.dot(X.T,delta2)\n",
    "        return dJdW1,dJdW2\n",
    "    \n",
    "    def costFunctionPrime_ReLU(self, X, y):\n",
    "        #Compute derivatives with respect to W1 and W2\n",
    "        self.yHat=self.forward_ReLU(X)\n",
    "        delta3 = np.multiply(-(y-self.yHat),self.reluPrime(self.z3))\n",
    "        dJdW2=np.dot(self.a2.T,delta3)\n",
    "        delta2=np.dot(delta3,self.W2.T)*self.reluPrime(self.z2)\n",
    "        dJdW1=np.dot(X.T,delta2)\n",
    "        return dJdW1,dJdW2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(([3,5],[5,1],[10,1]),dtype=float)\n",
    "y=np.array(([75],[80],[93]),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/np.amax(X,axis=0)\n",
    "y=y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yH=NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testValues=np.arange(-5,5,0.01)\n",
    "plt.plot(testValues,NN.sigmoid(testValues),linewidth=2)\n",
    "plt.plot(testValues, NN.sigmoidPrime(testValues),linewidth=2)\n",
    "plt.grid(1)\n",
    "plt.legend(['Sigmoid','SigmoidPrime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost1=NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJdW1,dJdW2=NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJdW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=1\n",
    "NN.W1 = NN.W1+scalar*dJdW1\n",
    "NN.W2 = NN.W2+scalar*dJdW2\n",
    "cost2 = NN.costFunction(X,y)\n",
    "print (cost1,cost2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Example\n",
    "Now, Let's keep updating weights until cost became lower than a threshold.\n",
    "We need to recalculate \n",
    "- partial L/w every time: ```dJdW1,dJdW2=NN.costFunctionPrime(X,y)```\n",
    "\n",
    "- weight: ```NN.W1 = NN.W1-scalar*dJdW1```\n",
    "\n",
    "- cost: ```cost = NN.costFunction(X,y)```\n",
    "\n",
    "Then, plot costs vs iteration with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=Neural_Network()\n",
    "scalar=3 # Learning Rate\n",
    "costs=[]\n",
    "index=0\n",
    "while True:\n",
    "    dJdW1,dJdW2=NN.costFunctionPrime_ReLU(X,y)\n",
    "    NN.W1 = NN.W1-scalar*dJdW1\n",
    "    NN.W2 = NN.W2-scalar*dJdW2\n",
    "    cost = NN.costFunction_ReLU(X,y)\n",
    "    costs.append(cost)\n",
    "    if(cost<0.001): break\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Gradient Descent Using ReLU\")\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=Neural_Network()\n",
    "scalar=3 # Learning Rate\n",
    "costs=[]\n",
    "index=0\n",
    "while True:\n",
    "    dJdW1,dJdW2=NN.costFunctionPrime(X,y)\n",
    "    NN.W1 = NN.W1-scalar*dJdW1\n",
    "    NN.W2 = NN.W2-scalar*dJdW2\n",
    "    cost = NN.costFunction(X,y)\n",
    "    costs.append(cost)\n",
    "    if(cost<0.001): break\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Gradient Descent Using Sigmoid\")\n",
    "\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing between different scalar values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scalar_values = [1,10,50, 100]#[0.1, 1, 3, 10]\n",
    "colors = ['r', 'g', 'b', 'm']\n",
    "labels = [f'scalar={s}' for s in scalar_values]\n",
    "\n",
    "for scalar, color, label in zip(scalar_values, colors, labels):\n",
    "    NN = Neural_Network() \n",
    "    costs = []\n",
    "    \n",
    "    while True:\n",
    "        dJdW1, dJdW2 = NN.costFunctionPrime(X, y)\n",
    "        NN.W1 -= scalar * dJdW1\n",
    "        NN.W2 -= scalar * dJdW2\n",
    "        cost = NN.costFunction(X, y)\n",
    "        costs.append(cost)\n",
    "        if cost < 0.002:\n",
    "            break\n",
    "        if iter > 100000: # It might take too long time\n",
    "            break\n",
    "    \n",
    "    plt.plot(range(len(costs)), costs, color=color, label=label)\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Learning Curve by Scalar (Learning Rate)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scalar_values = [3]#[0.1, 1, 3, 10]\n",
    "colors = ['r', 'g', 'b', 'm']\n",
    "labels = [f'scalar={s}' for s in scalar_values]\n",
    "\n",
    "for scalar, color, label in zip(scalar_values, colors, labels):\n",
    "    NN = Neural_Network() \n",
    "    costs = []\n",
    "    iter = 0\n",
    "    while True:\n",
    "        dJdW1, dJdW2 = NN.costFunctionPrime_ReLU(X, y)\n",
    "        NN.W1 -= scalar * dJdW1\n",
    "        NN.W2 -= scalar * dJdW2\n",
    "        cost = NN.costFunction_ReLU(X, y)\n",
    "        costs.append(cost)\n",
    "        iter = iter+1\n",
    "        if cost < 0.001:\n",
    "            break\n",
    "        if iter > 100000: # It might take too long time\n",
    "            break\n",
    "    \n",
    "    plt.plot(range(len(costs)), costs, color=color, label=label)\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Learning Curve by Scalar (Learning Rate)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Layers\n",
    "\n",
    "- added one more hidden Layer.\n",
    "- it has 3 or 10 nodes (according to ```self.hiddenLayer2Size```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Neural_Network_addLayer(object):\n",
    "    def __init__(self):\n",
    "        #Define Parameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize=1\n",
    "        self.hiddenLayer1Size=3\n",
    "        self.hiddenLayer2Size=10\n",
    "        \n",
    "        #Define Weights\n",
    "        self.W1=np.random.rand(self.inputLayerSize,self.hiddenLayer1Size)\n",
    "        self.W2=np.random.rand(self.hiddenLayer1Size,self.hiddenLayer2Size)\n",
    "        self.W3=np.random.rand(self.hiddenLayer2Size,self.outputLayerSize)\n",
    "\n",
    "    def forward(self,X):\n",
    "        #Propagate inputs through network\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        self.a3 = self.sigmoid(self.z3)\n",
    "        self.z4 = np.dot(self.a3,self.W3)\n",
    "        yHat = self.sigmoid(self.z4)\n",
    "        return yHat\n",
    "    \n",
    "    def forward_ReLU(self,X):\n",
    "        #Propagate inputs through network\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        self.a3 = self.relu(self.z3)\n",
    "        self.z4 = np.dot(self.a3,self.W3)\n",
    "        yHat = self.sigmoid(self.z4)\n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply Sigmoid Activation Function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Derivative of Sigmoid Function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def reluPrime(self,z):\n",
    "        return 1 * (z > 0)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute Cost Function with weights already stored in class\n",
    "        self.yHat=self.forward(X)\n",
    "        J=0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def costFunction_ReLU(self, X, y):\n",
    "        #Compute Cost Function with weights already stored in class\n",
    "        self.yHat=self.forward_ReLU(X)\n",
    "        J=0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "    \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivatives with respect to W1 and W2\n",
    "        self.yHat=self.forward(X)\n",
    "        delta4 = np.multiply(-(y-self.yHat),self.sigmoidPrime(self.z4))\n",
    "        dJdW3=np.dot(self.a3.T,delta4)\n",
    "        delta3 = np.multiply(-(y-self.yHat),self.sigmoidPrime(self.z3))\n",
    "        dJdW2=np.dot(self.a2.T,delta3)\n",
    "        delta2=np.dot(delta3,self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1=np.dot(X.T,delta2)\n",
    "        return dJdW1,dJdW2,dJdW3\n",
    "    \n",
    "    def costFunctionPrime_ReLU(self, X, y):\n",
    "        #Compute derivatives with respect to W1 and W2\n",
    "        self.yHat=self.forward_ReLU(X)\n",
    "        delta4 = np.multiply(-(y-self.yHat),self.reluPrime(self.z4))\n",
    "        dJdW3=np.dot(self.a3.T,delta4)\n",
    "        delta3 = np.multiply(-(y-self.yHat),self.reluPrime(self.z3))\n",
    "        dJdW2=np.dot(self.a2.T,delta3)\n",
    "        delta2=np.dot(delta3,self.W2.T)*self.reluPrime(self.z2)\n",
    "        dJdW1=np.dot(X.T,delta2)\n",
    "        return dJdW1,dJdW2,dJdW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=Neural_Network_addLayer()\n",
    "scalar=0.1 # Learning Rate\n",
    "costs=[]\n",
    "index=0\n",
    "while True:\n",
    "    dJdW1,dJdW2,dJdW3=NN.costFunctionPrime_ReLU(X,y)\n",
    "    NN.W1 = NN.W1-scalar*dJdW1\n",
    "    NN.W2 = NN.W2-scalar*dJdW2\n",
    "    NN.W3 = NN.W3-scalar*dJdW3\n",
    "    cost = NN.costFunction_ReLU(X,y)\n",
    "    costs.append(cost)\n",
    "    index = index+1\n",
    "    if(cost<0.001): break\n",
    "    if(index>10000): break\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Gradient Descent Using ReLU\")\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scalar_values = [0.01, 0.1, 1, 3]\n",
    "colors = ['r', 'g', 'b', 'm']\n",
    "labels = [f'scalar={s}' for s in scalar_values]\n",
    "\n",
    "for scalar, color, label in zip(scalar_values, colors, labels):\n",
    "    NN = Neural_Network_addLayer() \n",
    "    costs = []\n",
    "    iter = 0\n",
    "    while True:\n",
    "        dJdW1,dJdW2,dJdW3=NN.costFunctionPrime_ReLU(X,y)\n",
    "        NN.W1 = NN.W1-scalar*dJdW1\n",
    "        NN.W2 = NN.W2-scalar*dJdW2\n",
    "        NN.W3 = NN.W3-scalar*dJdW3\n",
    "        cost = NN.costFunction_ReLU(X,y)\n",
    "        costs.append(cost)\n",
    "        iter = iter+1\n",
    "        if cost < 0.001:\n",
    "            break\n",
    "        if iter > 10000: \n",
    "            break\n",
    "    \n",
    "    plt.plot(range(len(costs)), costs, color=color, label=label)\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Learning Curve by Scalar (Learning Rate)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
